{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한글_영어_일본어 전처리",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1i6xYHcI3hLE-TiAoUfdTBVqPAzsW0ufE",
      "authorship_tag": "ABX9TyMlSYssTWEsMWg6cazw6v4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngdeuk-Koo/KNLP/blob/main/%ED%95%9C%EA%B8%80_%EC%98%81%EC%96%B4_%EC%9D%BC%EB%B3%B8%EC%96%B4_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "metadata": {
        "id": "lyLt1Jo98RRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "\n",
        "# path_train = '/content/drive/MyDrive/AI_Study/NLP/DATA/번역 Corpus/[원천]ko2ja_culture_training_csv.zip'\n",
        "# path_val = '/content/drive/MyDrive/AI_Study/NLP/DATA/번역 Corpus/[원천]ko2ja_culture_validation_csv.zip'\n",
        "\n",
        "# with zipfile.ZipFile(path_train, 'r') as zip_ref:\n",
        "#     zip_ref.extractall()\n",
        "\n",
        "# with zipfile.ZipFile(path_val, 'r') as zip_ref:\n",
        "#     zip_ref.extractall()"
      ],
      "metadata": {
        "id": "NA7c_7n2EYES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# train_auto = pd.read_csv('/content/ko2ja_auto_1_training.csv', encoding='utf-8')\n",
        "# train_edu = pd.read_csv('/content/ko2ja_edu_1_training.csv', encoding='utf-8')\n",
        "# train_finance = pd.read_csv('/content/ko2ja_finance_1_training.csv', encoding='utf-8')\n",
        "# train_folk = pd.read_csv('/content/ko2ja_folk_1_training.csv', encoding='utf-8')\n",
        "# train_tech = pd.read_csv('/content/ko2ja_it_tech_1_training.csv', encoding='utf-8')\n",
        "# train_kpop = pd.read_csv('/content/ko2ja_kpop_1_training.csv', encoding='utf-8')\n",
        "# train_medical = pd.read_csv('/content/ko2ja_medical_1_training.csv', encoding='utf-8')\n",
        "# train_patent = pd.read_csv('/content/ko2ja_patent_1_training.csv', encoding='utf-8')\n",
        "# train_society = pd.read_csv('/content/ko2ja_society_1_training.csv', encoding='utf-8')\n",
        "\n",
        "# validation_auto = pd.read_csv('/content/ko2ja_auto_2_validation.csv', encoding='utf-8')\n",
        "# validation_edu = pd.read_csv('/content/ko2ja_edu_2_validation.csv', encoding='utf-8')\n",
        "# validation_finance = pd.read_csv('/content/ko2ja_finance_2_validation.csv', encoding='utf-8')\n",
        "# validation_folk = pd.read_csv('/content/ko2ja_folk_2_validation.csv', encoding='utf-8')\n",
        "# validation_tech = pd.read_csv('/content/ko2ja_it_tech_2_validation.csv', encoding='utf-8')\n",
        "# validation_kpop = pd.read_csv('/content/ko2ja_kpop_2_validation.csv', encoding='utf-8')\n",
        "# validation_medical = pd.read_csv('/content/ko2ja_medical_2_validation.csv', encoding='utf-8')\n",
        "# validation_patent = pd.read_csv('/content/ko2ja_patent_2_validation.csv', encoding='utf-8')\n",
        "# validation_society = pd.read_csv('/content/ko2ja_society_2_validation.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "XjutNG2WF-Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/AI_Study/NLP/DATA/한글-영어 text/1113_social_train_set_1210529.csv', encoding='utf-8')\n",
        "val = pd.read_csv('/content/drive/MyDrive/AI_Study/NLP/DATA/한글-영어 text/1113_social_valid_set_151316.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "FrFxQZ14fQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat((train, val), axis=0)\n",
        "print(len(df))\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "rQioKJssgfHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus(src, lan='kor'):\n",
        "    import re\n",
        "\n",
        "    # try:    # 소문자가 있을시\n",
        "    #     src = src.lower()\n",
        "\n",
        "    # except: # 소문자가 없을시\n",
        "    #     pass\n",
        "\n",
        "    if lan == 'kor':\n",
        "        src = str(src).replace('・', ' ')\n",
        "        src = src.replace('·', ' ')\n",
        "        # src = src.replace('-', ' ')\n",
        "        src = re.sub(r'\\([^)]*\\)', '', src)\n",
        "        src = re.sub(r'\\[[^)]*\\]', '', src)\n",
        "        src = re.sub('[-=+#/\\:;^$@*\\’\"“”※&ㆍ_『』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'\"…》㎝「」]','', src)\n",
        "        src = re.sub(r\"([?!.,%〜∼-])\", r\" \\1 \", src)\n",
        "        src = re.sub('[0-9] \\. [0-9]', '1.4', src)\n",
        "        src = re.sub('[0-9] , [0-9]', '1,4', src)\n",
        "        # src = re.sub(' \\. ', '', src)\n",
        "        return src\n",
        "\n",
        "\n",
        "    if lan == 'eng':\n",
        "        src = str(src).replace('・', ' ')\n",
        "        src = src.replace('、', ', ')\n",
        "        src = src.replace('·', ' ')\n",
        "        # src = src.replace('-', ' ')\n",
        "        src = re.sub(r'\\([^)]*\\)', '', src)\n",
        "        src = re.sub(r'\\[[^)]*\\]', '', src)\n",
        "        src = re.sub('[-=+#/\\:;^$@*\\’\"“”※&ㆍ_『』\\\\‘|\\(\\)\\[\\]\\<\\>`\\\"…》㎝「」]','', src)\n",
        "        src = re.sub(r\"([?!〜.%〜∼-])\", r\" \\1 \", src)\n",
        "        # src = re.sub('[0-9] \\. [0-9]', '1.4', src)\n",
        "        # src = re.sub('[0-9] , [0-9]', '1,4', src)\n",
        "        # src = re.sub(' \\. ', '', src)   \n",
        "        return src"
      ],
      "metadata": {
        "id": "8D2TTOSQCIVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Clean_data_generation(train, sentence1=None, sentence2=None, null_check=False, duplicates_check=False, text_preprocessing=False, sentence_length_count_check=False, sentence_trunk=False):\n",
        "\n",
        "    print('Data Size Check :', len(train), '\\n')\n",
        "\n",
        "    ex_len = len(train)\n",
        "\n",
        "    if null_check == True :\n",
        "\n",
        "        if (False in train[sentence1].isnull()) or (False in train[sentence2].isnull())== False:\n",
        "            print('>> Null값이 존재하지 않습니다', '\\n')\n",
        "\n",
        "        else:\n",
        "            train.dropna(how='any')\n",
        "            print('>> Null값이 제거 됐습니다', '\\n')\n",
        "            print('>> Data Size check :', len(train), '\\n')\n",
        "\n",
        "    else :\n",
        "        print('>> Null값을 제거하지 않습니다', '\\n')\n",
        "\n",
        "    if duplicates_check == True :\n",
        "\n",
        "        if (True in train.duplicated([sentence1, sentence2], keep='first')) == True:\n",
        "            print('>> 중복값이 존재하지 않습니다', '\\n')\n",
        "\n",
        "        else:\n",
        "            train.drop_duplicates(subset=[sentence1, sentence2], keep='first', inplace=True, ignore_index=False)\n",
        "            print('>> 중복값이 제거 됐습니다', '\\n')\n",
        "            print('>> 제거된 중복값 갯수 :', (ex_len - len(train)), '\\n')\n",
        "            print('>> Data Size check :', len(train), '\\n')\n",
        "\n",
        "    else :\n",
        "        print('>> 중복값을 제거하지 않습니다', '\\n')\n",
        "\n",
        "    if text_preprocessing == True:\n",
        "        print('>> Text 전처리를 진행중 입니다', '\\n')\n",
        "        train[sentence1] = train[sentence1].apply(lambda x:build_corpus(x, lan='kor'))\n",
        "        train[sentence2] = train[sentence2].apply(lambda x:build_corpus(x, lan='eng'))\n",
        "\n",
        "    else:\n",
        "        print('>> Text 전처리를 진행하지 않습니다', '\\n')\n",
        "\n",
        " \n",
        "    if sentence_length_count_check == True:\n",
        "        print('>> 그래프를 출력합니다', '\\n')\n",
        "\n",
        "        kor = train[sentence1]\n",
        "        jap = train[sentence2]\n",
        "\n",
        "        k_max_len = max([len(w) for w in kor])\n",
        "        k_min_len = min([len(w) for w in kor])\n",
        "        j_max_len = max([len(w) for w in jap])\n",
        "        j_min_len = min([len(w) for w in jap])\n",
        "\n",
        "\n",
        "        kor_length = np.zeros((k_max_len), dtype=np.uint)\n",
        "        jap_length = np.zeros((j_max_len), dtype=np.uint)\n",
        "\n",
        "        for k_sen, e_sen in zip(kor, jap): #중복이 제거된 코퍼스\n",
        "            kor_length[len(k_sen)-1] += 1\n",
        "            jap_length[len(e_sen)-1] += 1\n",
        "\n",
        "\n",
        "        plt.figure(figsize=[18, 3])\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Korea Sentence Length Distribution\")\n",
        "        plt.bar(range(k_max_len), kor_length, width=1.0)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.figure(figsize=[18, 3])\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(range(j_max_len), jap_length, width=1.0)\n",
        "        plt.title(\"English Sentence Length Distribution\")\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        \n",
        "        print()\n",
        "        print('>> 한국어 문장 최장 길이 :', k_max_len, '\\n')\n",
        "        print('>> 한국어 문장 최단 길이 :', k_min_len, '\\n')\n",
        "        print('>> 영어 문장 최장 길이 :', j_max_len, '\\n')\n",
        "        print('>> 영어 문장 최장 길이 :', j_min_len, '\\n') \n",
        "\n",
        "\n",
        "    else:\n",
        "        print('>> 그래프를 출력하지 않습니다', '\\n')     \n",
        "\n",
        "    if sentence_trunk == True:\n",
        "\n",
        "        print('>> 문장을 자르고 있습니다', '\\n')\n",
        "\n",
        "        train['k_sen_len'] = train[sentence1].str.len()\n",
        "        train['j_sen_len'] = train[sentence2].str.len()\n",
        "\n",
        "        train = train[(train['k_sen_len'] <= 100) & (train['j_sen_len'] <= 250)]\n",
        "        train = train[(train['k_sen_len'] >= 30) & (train['j_sen_len'] >= 60)]\n",
        "\n",
        "        print('Data Size Check :', len(train), '\\n')\n",
        "\n",
        "        kor = train[sentence1]\n",
        "        jap = train[sentence2]\n",
        "\n",
        "        k_max_len = max([len(w) for w in kor])\n",
        "        k_min_len = min([len(w) for w in kor])\n",
        "        j_max_len = max([len(w) for w in jap])\n",
        "        j_min_len = min([len(w) for w in jap])\n",
        "\n",
        "\n",
        "        kor_length = np.zeros((k_max_len), dtype=np.uint)\n",
        "        jap_length = np.zeros((j_max_len), dtype=np.uint)\n",
        "\n",
        "        for k_sen, e_sen in zip(kor, jap): #중복이 제거된 코퍼스\n",
        "            kor_length[len(k_sen)-1] += 1\n",
        "            jap_length[len(e_sen)-1] += 1\n",
        "\n",
        "        print('>> 그래프를 출력합니다', '\\n')\n",
        "\n",
        "        plt.figure(figsize=[18, 3])\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Korea Sentence Length Distribution\")\n",
        "        plt.bar(range(k_max_len), kor_length, width=1.0)\n",
        "        plt.grid()\n",
        "\n",
        "        plt.figure(figsize=[18, 3])\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(range(j_max_len), jap_length, width=1.0)\n",
        "        plt.title(\"Japan Sentence Length Distribution\")\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        \n",
        "        print()\n",
        "        print('>> 전처리된 한국어 문장 최장 길이 :', k_max_len, '\\n')\n",
        "        print('>> 전처리된 한국어 문장 최단 길이 :', k_min_len, '\\n')\n",
        "        print('>> 전처리된 영어 문장 최장 길이 :', j_max_len, '\\n')\n",
        "        print('>> 전처리된 영어 문장 최장 길이 :', j_min_len, '\\n') \n",
        "\n",
        "    else:\n",
        "        print('>> 문장을 자르지 않습니다', '\\n') \n",
        "\n",
        "    return train\n",
        "\n",
        "\n",
        "df = Clean_data_generation(df, \n",
        "                           sentence1='ko',\n",
        "                           sentence2='en',\n",
        "                           null_check=True, \n",
        "                           duplicates_check=True,\n",
        "                           text_preprocessing=True, \n",
        "                           sentence_length_count_check=True,\n",
        "                           sentence_trunk=True)"
      ],
      "metadata": {
        "id": "YrisuKG4vQ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 불러오기"
      ],
      "metadata": {
        "id": "vQOceFkRAlkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_kor = [w for w in df['ko']]\n",
        "clean_eng = [w for w in df['en']]"
      ],
      "metadata": {
        "id": "nXuTAIVsOhq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in zip(clean_kor, clean_eng):\n",
        "#     print(i)"
      ],
      "metadata": {
        "id": "8a-oRlbBIX0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_max_len = max([len(w) for w in clean_kor])\n",
        "k_min_len = min([len(w) for w in clean_kor])\n",
        "j_max_len = max([len(w) for w in clean_eng])\n",
        "j_min_len = min([len(w) for w in clean_eng])\n",
        "print(k_max_len, j_max_len)\n",
        "print(k_min_len, j_min_len)\n",
        "\n",
        "kor_length = np.zeros((k_max_len), dtype=np.uint)\n",
        "jap_length = np.zeros((j_max_len), dtype=np.uint)\n",
        "\n",
        "for k_sen, e_sen in zip(clean_kor, clean_eng): #중복이 제거된 코퍼스\n",
        "    kor_length[len(k_sen)-1] += 1\n",
        "    jap_length[len(e_sen)-1] += 1\n",
        "\n",
        "\n",
        "plt.figure(figsize=[18, 3])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Korea Sentence Length Distribution\")\n",
        "plt.bar(range(k_max_len), kor_length, width=1.0)\n",
        "plt.grid()\n",
        "\n",
        "plt.figure(figsize=[18, 3])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(j_max_len), jap_length, width=1.0)\n",
        "plt.title(\"Japan Sentence Length Distribution\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uy8vKo1jH__V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전처리 언어 저장"
      ],
      "metadata": {
        "id": "3Pcf4VsQ2HdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_drive_path = '/content/drive/MyDrive/AI_Study/NLP/DATA/한글-영어 text/'\n",
        "\n",
        "with open(g_drive_path + 'clean_ko_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(map(str, clean_kor)))\n",
        "\n",
        "with open(g_drive_path + 'clean_en_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(map(str, clean_eng)))"
      ],
      "metadata": {
        "id": "_okb2WgASj4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}