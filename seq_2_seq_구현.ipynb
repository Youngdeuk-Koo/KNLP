{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngdeuk-Koo/KNLP/blob/main/seq_2_seq_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nTAYx5aSZc9"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "%cd ../\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0.프로젝트 진행중 생겼던 문제점과 해결법\n",
        "\n",
        "###1.크롤링 후 압축푸는거만 반나절 걸림(...)\n",
        "    1.1 url을 제대로 확인 하지 않아 빈 압축파일을 생성하고 있었음\n",
        "    1.2 url 확인 후 해결 \n",
        " \n",
        "###2.어떤 문자를 제거 하고 어떤 문자는 살려야 하는가?\n",
        "\n",
        "    2.1 ('&#8212;, ¡°I, ¡°, ¡± (CNN))등 불용어 제거\n",
        "\n",
        "    2.2 [. , ' \"] 만 남겨 봤으나 결과 좋지 않음 (Loss 4.0 ~ 5.0)\n",
        "\n",
        "    2.2 문자와 숫자만 남겨 봤으나 조금 좋아 졌을뿐 결과는 큰 의미 없음 (Loss 1.0 ~ 3.0)\n",
        "\n",
        "    2.4 한글과 영어 외에 모든 문자 삭제시 가장 좋은 결과 (Loss 0.01 ~ 0.03)\n",
        "\n",
        "###3.VOCAB_SIZE로 인한 GPU OOM에러\n",
        "    \n",
        "    3.1 문장이나 토큰의 길이와 상관없이 VOCAB_SIZE가 14,000이 넘으면 에러\n",
        "\n",
        "    3.2 적당한 VOCAB_SIZE를 위한 문장길이들만 추출\n",
        "\n",
        "    3.3 패딩 격차를 줄이기 위해 10~80개의 문장들만 추출\n",
        "\n",
        "    3.4 VOCAB_SIZE 13,000으로 시작\n",
        "\n",
        "###4.과적합과 로스 폭주를 막기 위한 적당한 에포크 찾기\n",
        "\n",
        "    4.1 에포크 10 (택도 없음)\n",
        "\n",
        "    4.2 에포크 20 (부족함)\n",
        "\n",
        "    4.3 에포크 40 (로스율 0.007 과적합이라 생각됨) or 로스율 폭주(Loss가 0.01에서 5.1로 폭주)\n",
        "\n",
        "    4.4 에포크 35 로스율 0.02때로 적당하다 판단\n",
        "\n",
        "###5.새로 입력되는 문장 패딩의 오류\n",
        "\n",
        "    5.1 지속되는 차원 불일치 오류(어디서 에러가 생기는지 확인이 쉽지 않았음)\n",
        "\n",
        "    5.2 입력 문장의 패딩쪽에서 에러가 생기는걸 발견\n",
        "\n",
        "    5.3 입력문장의 패딩이 [27, 80, 55, 0 .... ,0]이 아닌 [[27, 0, 0, ... ,0], [80, 0, 0, ... ,0], [55, 0, 0, ... ,0]]로 차원 오류가 생겼음\n",
        "\n",
        "    5.4 입력되는 토큰화의 리스트 플래튼 후 재정립\n",
        "\n",
        "    5.5 문제 해결\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qks_e46SmxPl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WwJHh3MSccc"
      },
      "source": [
        "#1.데이터 크롤링후 압축해제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrw5arQjSxLp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "\n",
        "download_root = 'https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/'\n",
        "train_url = download_root + 'korean-english-park.train.tar.gz'\n",
        "test_url = download_root + 'korean-english-park.test.tar.gz'\n",
        "\n",
        "_PATH = os.path.join('kor_eng_LangFile') \n",
        "\n",
        "def fetch_spam_data(spam_url=test_url, ham_url=train_url, spam_path=_PATH):\n",
        "    if not os.path.isdir(spam_path):\n",
        "        os.makedirs(spam_path)\n",
        "    for filename, url in ((\"Korean-english-park.train.tar.gz\", ham_url), (\"Korean-english-park.test.tar.gz\", spam_url)):\n",
        "        path = os.path.join(spam_path, filename)\n",
        "        if not os.path.isfile(path):\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            \n",
        "        # 압축해제\n",
        "        tar_gz_file = tarfile.open(path)\n",
        "        tar_gz_file.extractall(path=_PATH)\n",
        "        tar_gz_file.close()\n",
        "\n",
        "fetch_spam_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rmRikRK0fhw"
      },
      "source": [
        "#2.데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbiRzWwEV9zv",
        "outputId": "ce969f29-89df-412e-e5b9-4cfca22a1efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 길이 : 94123\n",
            "영어 길이 : 94123\n"
          ]
        }
      ],
      "source": [
        "with open('/content/kor_eng_LangFile/korean-english-park.train.ko', \"r\") as f:\n",
        "    kor = f.read().splitlines()\n",
        "\n",
        "with open('/content/kor_eng_LangFile/korean-english-park.train.en', \"r\") as f:\n",
        "    eng = f.read().splitlines()\n",
        "\n",
        "print('한글 길이 :', len(kor))\n",
        "print('영어 길이 :', len(eng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGGqAinS0jfW"
      },
      "source": [
        "#3.중복제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td2AvXt1BNxT",
        "outputId": "b97b036d-827d-4d30-f0cc-36a128148ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "중복 제거후 문장 길이 : 78968\n",
            "\n",
            "문장 일부 확인 :\n",
            "[('카수리 외무장관은 CNN과의 인터뷰에서 “파키스탄과 아프가니스탄 국경 인근 부족 거주 산악지역에 8만5000명의 병력을 이미 배치했다”고 말했다.', 'Khurshid Kasuri further told CNN\\'s \"Late Edition with Wolf Blitzer\" that 85,000 Pakistani troops were already in the Federally Administered Tribal Areas, a mountainous and notoriously dangerous region at the porous border between Pakistan and Afghanistan.'), ('TAIPEI, Taiwan (CNN) / 이수지(JOINS)', 'Protests mark China envoy`s Taiwan trip')]\n"
          ]
        }
      ],
      "source": [
        "pairs = [w for w in zip(kor, eng)]\n",
        "pair = list(set(pairs))\n",
        "print('중복 제거후 문장 길이 :', len(pair))\n",
        "print()\n",
        "print('문장 일부 확인 :')\n",
        "print(pair[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-d7LV200rAz"
      },
      "source": [
        "#4.문장 길이 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAsxre6qAUcc",
        "outputId": "cb4b8b79-8e35-45fd-ea28-325f7145bf14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장의 최단 길이 : 1\n",
            "문장의 최장 길이 : 377\n",
            "문장의 최단 길이 : 1\n",
            "문장의 최장 길이 : 605\n"
          ]
        }
      ],
      "source": [
        "k_min_len = 999\n",
        "k_max_len = 0\n",
        "\n",
        "e_min_len = 999\n",
        "e_max_len = 0\n",
        "\n",
        "k_sum_len = 0\n",
        "e_sum_len = 0\n",
        "\n",
        "for k_sen, e_sen in pair:\n",
        "    k_length = len(k_sen)\n",
        "    e_length = len(e_sen)\n",
        "    if k_min_len > k_length : k_min_len = k_length\n",
        "    if e_min_len > e_length : e_min_len = e_length\n",
        "\n",
        "    if k_max_len < k_length : k_max_len = k_length\n",
        "    if e_max_len < e_length : e_max_len = e_length\n",
        "    k_sum_len += k_length\n",
        "    e_sum_len += e_length\n",
        "\n",
        "\n",
        "print(\"문장의 최단 길이 :\", k_min_len)\n",
        "print(\"문장의 최장 길이 :\", k_max_len)\n",
        "\n",
        "print(\"문장의 최단 길이 :\", e_min_len)\n",
        "print(\"문장의 최장 길이 :\", e_max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCVyj6-0yxd"
      },
      "source": [
        "#5.문장 길이 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "sJMko9ozHQ1A",
        "outputId": "b3875127-6b5d-425c-b998-cd5826925a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kor_length = np.zeros((k_max_len), dtype=np.uint)\n",
        "eng_length = np.zeros((e_max_len), dtype=np.uint)\n",
        "\n",
        "for k_sen, e_sen in pair: #중복이 제거된 코퍼스\n",
        "    kor_length[len(k_sen)-1] += 1\n",
        "    eng_length[len(e_sen)-1] += 1\n",
        "\n",
        "\n",
        "plt.figure(figsize=[18, 3])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Korea Sentence Length Distribution\")\n",
        "plt.bar(range(k_max_len), kor_length, width=1.0)\n",
        "plt.grid()\n",
        "\n",
        "plt.figure(figsize=[18, 3])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(e_max_len), eng_length, width=1.0)\n",
        "plt.title(\"English Sentence Length Distribution\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yjg2PoQ08p-"
      },
      "source": [
        "#6.토크나이징 및 문장 전처리(진행중)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e92Z0xjBVCbo"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "import re\n",
        "\n",
        "def preprocess_sentence(sentence, token=False):\n",
        "    if token == False:      \n",
        "        sentence = sentence.replace('&#8212;', '')\n",
        "        sentence = sentence.replace('¡°I', '')\n",
        "        sentence = sentence.replace('¡°', '')\n",
        "        sentence = sentence.replace('¡±', '')\n",
        "        sentence = sentence.replace('¡¯', '')\n",
        "        sentence = re.sub('[-=+,#/\\?:;^$.@*\\’\"“”※~&%ㆍ_!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》.]','', sentence)\n",
        "        sentence = re.sub(r\"[^가-힣 ]\", \"\", sentence)\n",
        "\n",
        "        return sentence\n",
        "    \n",
        "    if token == True:\n",
        "        sentence = sentence.lower().strip()\n",
        "        sentence = sentence.replace('&#8212;', '')\n",
        "        sentence = sentence.replace('¡°I', '')\n",
        "        sentence = sentence.replace('¡°', '')\n",
        "        sentence = sentence.replace('¡±', '')\n",
        "        sentence = sentence.replace('¡¯', '')      \n",
        "        sentence = re.sub('[-=+,#/\\?:;^$.@*\\’\"“”※~&%ㆍ_!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》.]','', sentence)\n",
        "        sentence = re.sub(r\"[^a-z ]\", \"\", sentence)\n",
        "        sentence = sentence.strip()\n",
        "        sentence = '<start> ' + sentence\n",
        "        sentence += ' <end>'\n",
        "\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzdyPft1E24"
      },
      "source": [
        "#7.분포에 맞는 문장 길이 선택"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6e-qWeZYePp"
      },
      "outputs": [],
      "source": [
        "kor_corpus = []\n",
        "eng_corpus = []\n",
        "mecab = Mecab()\n",
        "num_examples = len(pair)\n",
        "\n",
        "for kor, eng in pair[:num_examples]:\n",
        "    k_cop = preprocess_sentence(kor, token=False)     \n",
        "    e_cop = preprocess_sentence(eng, token=True) \n",
        "    m_k_cop = mecab.morphs(k_cop)\n",
        "\n",
        "    if 10 <= len(m_k_cop) <= 80 and 15 <= len(e_cop) <= 85:       # 보캡 사이즈 조절                \n",
        "        kor_corpus.append(m_k_cop) \n",
        "        eng_corpus.append(e_cop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdgQH_GNZqn9",
        "outputId": "2f516224-0815-40b2-8850-6e758f646404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 확인 : ['강도', '는', '이런', '대응', '을', '전혀', '예상', '하', '지', '못한', '듯', '했', '다']\n",
            "영어 확인 : <start> the guy obviously was not expecting such a reaction <end>\n",
            "\n",
            "한글 문장 갯수 : 10042\n",
            "영어 문장 갯수 : 10042\n",
            "\n",
            "한글 문장 최장 길이 : 73\n",
            "한글 문장 최단 길이 : 10\n",
            "영어 문장 최장 길이 : 85\n",
            "영어 문장 최단 길이 : 22\n"
          ]
        }
      ],
      "source": [
        "print(\"한글 확인 :\", kor_corpus[3000])\n",
        "print(\"영어 확인 :\", eng_corpus[3000])\n",
        "print()\n",
        "print(\"한글 문장 갯수 :\", len(kor_corpus))\n",
        "print(\"영어 문장 갯수 :\", len(eng_corpus))\n",
        "print()\n",
        "\n",
        "k_maxlen = max([len(w) for w in kor_corpus])\n",
        "e_maxlen = max([len(w) for w in eng_corpus])\n",
        "\n",
        "k_minlen = min([len(w) for w in kor_corpus])\n",
        "e_minlen = min([len(w) for w in eng_corpus])\n",
        "\n",
        "print('한글 문장 최장 길이 :', k_maxlen)\n",
        "print('한글 문장 최단 길이 :', k_minlen)\n",
        "print('영어 문장 최장 길이 :', e_maxlen)\n",
        "print('영어 문장 최단 길이 :', e_minlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfPJxbX21bLg"
      },
      "source": [
        "#8.전처리 확인용 텍스트 파일 생성(진행중)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyee3X4kjcni"
      },
      "outputs": [],
      "source": [
        "with open('한글 전처리 확인용', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(map(str, kor_corpus)))\n",
        "\n",
        "with open('영어 전처리 확인용', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(map(str, eng_corpus)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM-3elbq1mhR"
      },
      "source": [
        "#9.문장 토큰화 및 보캡사이즈 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz_vMiH3e3cT"
      },
      "outputs": [],
      "source": [
        "def tokenize(corpus, maxlen):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return tensor, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsEJvk5-fDER",
        "outputId": "5c60489b-1243-41b3-b0fd-3e8e1406dd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Korean Vocab Size : 14257\n",
            "English Vocab Size : 12977\n"
          ]
        }
      ],
      "source": [
        "kor_tensor, kor_tokenizer = tokenize(kor_corpus, k_maxlen)\n",
        "eng_tensor, eng_tokenizer = tokenize(eng_corpus, e_maxlen)\n",
        "\n",
        "print(\"Korean Vocab Size :\", len(kor_tokenizer.index_word))\n",
        "print(\"English Vocab Size :\", len(eng_tokenizer.index_word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyWy1mw_sL5Q",
        "outputId": "6588941a-67fa-4d8b-c9f4-34b09da3170d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 패딩 길이 : 73\n",
            "영어 패딩 길이 : 85\n"
          ]
        }
      ],
      "source": [
        "k_t_maxlen = max([len(w) for w in kor_tensor])\n",
        "e_t_maxlen = max([len(w) for w in eng_tensor])\n",
        "\n",
        "print('한글 패딩 길이 :', k_t_maxlen)\n",
        "print('영어 패딩 길이 :', e_t_maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-zxyMK413E4"
      },
      "source": [
        "#10.데이터 셋 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilvSHtnJfxqX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "kor_train, kor_val, eng_train, eng_val = train_test_split(kor_tensor, eng_tensor, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAnBv_Oi18pM"
      },
      "source": [
        "#11.바다나우 어텐션 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pUWlUb8gUX_"
      },
      "outputs": [],
      "source": [
        "# 바다나우 어텐션 클래스 만들기\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_dec = tf.keras.layers.Dense(units)\n",
        "        self.w_enc = tf.keras.layers.Dense(units)\n",
        "        self.w_com = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_enc, h_dec):\n",
        "        # h_enc shape : [batch x length x units]\n",
        "        # h_dec shape : [batch x units]\n",
        "\n",
        "        h_enc = self.w_enc(h_enc)\n",
        "        # print('한글 인코더 출력 값 행렬 :', h_enc)\n",
        "        # print()\n",
        "        # print('차원 추가 전 한글 끝 시퀀스 행렬 :', h_dec)\n",
        "        # print()\n",
        "        h_dec = tf.expand_dims(h_dec, 1)\n",
        "        h_dec = self.w_dec(h_dec)\n",
        "        # print('차원 추가 후 한글 끝 시퀀스 행렬 :', h_dec)\n",
        "        # print()\n",
        "\n",
        "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
        "        # print('컴바인 Dense[1](하.탄젠트(끝 시퀀스 + 인코더 출력)) 행렬 :', score)\n",
        "        # print()\n",
        "\n",
        "        attn = tf.nn.softmax(score, axis = 1)\n",
        "        # print('attn(소프트 맥스 처리된) 행렬 :', attn)\n",
        "        # print()\n",
        "\n",
        "        context_vec = attn * h_enc\n",
        "        # print('context_vec(attn * 인코더 출력)) 행렬 :', context_vec)\n",
        "        # print()\n",
        "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
        "        # print('다 더한 context_vec 행렬 :', context_vec)\n",
        "        # print()\n",
        "        \n",
        "        return context_vec, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSU2r-gv2DJI"
      },
      "source": [
        "#11.인코더 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpZp1F0ugU8n"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        # print('한글 토큰 입력 행렬 :', x)\n",
        "        # print()\n",
        "        out = self.embedding(x)\n",
        "        # print('한글 토큰 임베딩후 행렬 :', out )\n",
        "        # print()\n",
        "        out = self.gru(out)\n",
        "        # print('한글 토큰 GRU후 행렬 :', out)\n",
        "        # print()\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfJV41QE2GsK"
      },
      "source": [
        "#12.디코더 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54YhiBaNgWcc"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units= dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state= True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, h_dec, enc_out):\n",
        "        context_vec, attn = self.attention(enc_out, h_dec)\n",
        "        # print('다 더한 context_vec 행렬 :', context_vec)\n",
        "        # print()\n",
        "        # print('attn(소프트 맥스 처리된) 행렬 :', attn)\n",
        "        # print()\n",
        "        # print('영어 토큰 입력 행렬 :', x)\n",
        "        # print()\n",
        "        out = self.embedding(x)\n",
        "        # print('영어 토큰 임베딩후 행렬 :', out)\n",
        "        # print()\n",
        "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
        "        # print('context_vec와 임베딩한 영어 concat 행렬  :', out)\n",
        "        # print()\n",
        "\n",
        "        out, h_dec = self.gru(out)\n",
        "        # print('GRU를 거친 concat 행렬 :', out)\n",
        "        # print()\n",
        "        # print('GRU후 return_state 행렬 :', h_dec)\n",
        "        # print()\n",
        "        out = tf.reshape(out, (-1, out.shape[2]))\n",
        "        # print('0, 1 차원을 모두 곱한 reshape 행렬 :', out)\n",
        "        # print()\n",
        "        out = self.fc(out)\n",
        "        # print('최종 Dense 행렬 :', out)\n",
        "        # print()\n",
        "\n",
        "        return out, h_dec, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhCpPPCR2Ix3"
      },
      "source": [
        "#13.인코더 및 디코더 생성 후 샘플 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3bvfFKMgXq2",
        "outputId": "9038489d-fd5a-41e0-9e5b-5923a773d5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 인코더 출력 값 : (64, 30, 1024)\n",
            "샘플 스테이트 출력 값 : (64, 1024)\n",
            "샘플 디코더 출력 값 : (64, 12978)\n",
            "샘플 디코더 히든 스테이트 값 : (64, 1024)\n",
            "어텐션 값 : (64, 30, 1)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "src_vocab_size = len(kor_tokenizer.index_word)+1\n",
        "tgt_vocab_size = len(eng_tokenizer.index_word)+1\n",
        "\n",
        "units = 1024\n",
        "embedding_dim = 512\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(tgt_vocab_size, embedding_dim, units)\n",
        "\n",
        "# sample input\n",
        "sequence_len = 30\n",
        "\n",
        "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
        "sample_output = encoder(sample_enc)\n",
        "\n",
        "print('샘플 인코더 출력 값 :', sample_output.shape)\n",
        "\n",
        "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
        "print('샘플 스테이트 출력 값 :', sample_state.shape)\n",
        "\n",
        "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_state, sample_output)\n",
        "\n",
        "print('샘플 디코더 출력 값 :', sample_logits.shape)\n",
        "print('샘플 디코더 히든 스테이트 값 :', h_dec.shape)\n",
        "print('어텐션 값 :', attn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixlQnbAM2W42"
      },
      "source": [
        "#14.손실함수 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5uFIlaZgnRH"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype = loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnBKXSd2gPp"
      },
      "source": [
        "#15.Train 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfhWcYIfgocx"
      },
      "outputs": [],
      "source": [
        "@tf.function # 가속 연산\n",
        "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
        "    bsz = src.shape[0]                # kor_train 행의 갯수 (2228개)\n",
        "    loss = 0                          # 손실 시작 0\n",
        "\n",
        "    with tf.GradientTape() as tape:   # 학습하면서 발생한 모든 연산을 기록하는 테이프\n",
        "        enc_out = encoder(src)        # 인코딩된 kor_train / shape(2228, 57, 1024)\n",
        "        h_dec = enc_out[:, -1]        # 인코딩된 kor_train의 각 문장의 끝 시퀀스만 추출 / shape(2228, 1024)\n",
        "        \n",
        "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)  # 문장의 수(2228개)만큼 '<start>'의 인덱스[1] 로 된 (2228, 1) 차원 생성                                                                          \n",
        "\n",
        "        for t in range(1, tgt.shape[1]):                         # 1부터 eng_train의 열(문장내 단어수[60]) 만큼 반복 할때\n",
        "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)    # 인덱스[1] (2228, 1), 문장의 끝 시퀀스(2228, 1024), 인코딩된 한글(2228, 57, 1024)\n",
        "\n",
        "            loss += loss_function(tgt[:, t], pred)               # 영어 토큰과 예측값 비교하여 loss에 누적한다\n",
        "            dec_src = tf.expand_dims(tgt[:, t], 1)               # eng_train의 차원(2228, )을 증가(2228, 1)시킨 후 dec_src로 업데이트\n",
        "        \n",
        "    batch_loss = (loss / int(tgt.shape[1]))                      # 누적된 로스를 문장의 길이로 나누어 배치 로스를 구한다\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhcb0vURfIVd"
      },
      "source": [
        "#16.Test 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3c5NxZxd69M"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def eval_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
        "    bsz = src.shape[0]\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_out = encoder(src)\n",
        "        h_dec = enc_out[:, -1]\n",
        "\n",
        "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
        "\n",
        "        for t in range(1, tgt.shape[1]):\n",
        "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
        "\n",
        "            loss += loss_function(tgt[:, t], pred)\n",
        "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(tgt.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReO37Qqhjz2x"
      },
      "source": [
        "#17. 전체 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVOp2Aj3hWkn",
        "outputId": "a399668f-0577-44e0-ca0b-08a313919985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch  1: 100%|██████████| 126/126 [04:20<00:00,  2.07s/it, Loss 0.9219]\n",
            "Test Epoch  1: 100%|██████████| 32/32 [02:24<00:00,  4.53s/it, Test Loss 0.8928]\n",
            "Train Epoch  2: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.8610]\n",
            "Test Epoch  2: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.8226]\n",
            "Train Epoch  3: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.7843]\n",
            "Test Epoch  3: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.7386]\n",
            "Train Epoch  4: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.7226]\n",
            "Test Epoch  4: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.6798]\n",
            "Train Epoch  5: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.6677]\n",
            "Test Epoch  5: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.6300]\n",
            "Train Epoch  6: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.6125]\n",
            "Test Epoch  6: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.5738]\n",
            "Train Epoch  7: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.5550]\n",
            "Test Epoch  7: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.5160]\n",
            "Train Epoch  8: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.4972]\n",
            "Test Epoch  8: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.4599]\n",
            "Train Epoch  9: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.4423]\n",
            "Test Epoch  9: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.4149]\n",
            "Train Epoch 10: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.3908]\n",
            "Test Epoch 10: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.3653]\n",
            "Train Epoch 11: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.3412]\n",
            "Test Epoch 11: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.3210]\n",
            "Train Epoch 12: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.2955]\n",
            "Test Epoch 12: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.2778]\n",
            "Train Epoch 13: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.2545]\n",
            "Test Epoch 13: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.2423]\n",
            "Train Epoch 14: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.2193]\n",
            "Test Epoch 14: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.2096]\n",
            "Train Epoch 15: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.1886]\n",
            "Test Epoch 15: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.1793]\n",
            "Train Epoch 16: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.1618]\n",
            "Test Epoch 16: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.1555]\n",
            "Train Epoch 17: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.1403]\n",
            "Test Epoch 17: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.1320]\n",
            "Train Epoch 18: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.1196]\n",
            "Test Epoch 18: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.1140]\n",
            "Train Epoch 19: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.1025]\n",
            "Test Epoch 19: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0960]\n",
            "Train Epoch 20: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0868]\n",
            "Test Epoch 20: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0822]\n",
            "Train Epoch 21: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0736]\n",
            "Test Epoch 21: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0715]\n",
            "Train Epoch 22: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0627]\n",
            "Test Epoch 22: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0610]\n",
            "Train Epoch 23: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0541]\n",
            "Test Epoch 23: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0514]\n",
            "Train Epoch 24: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0469]\n",
            "Test Epoch 24: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0445]\n",
            "Train Epoch 25: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0398]\n",
            "Test Epoch 25: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0386]\n",
            "Train Epoch 26: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0342]\n",
            "Test Epoch 26: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0320]\n",
            "Train Epoch 27: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0295]\n",
            "Test Epoch 27: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0280]\n",
            "Train Epoch 28: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0256]\n",
            "Test Epoch 28: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0247]\n",
            "Train Epoch 29: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0223]\n",
            "Test Epoch 29: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.0217]\n",
            "Train Epoch 30: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0203]\n",
            "Test Epoch 30: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0182]\n",
            "Train Epoch 31: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0183]\n",
            "Test Epoch 31: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.0172]\n",
            "Train Epoch 32: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0161]\n",
            "Test Epoch 32: 100%|██████████| 32/32 [00:25<00:00,  1.27it/s, Test Loss 0.0160]\n",
            "Train Epoch 33: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0152]\n",
            "Test Epoch 33: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0152]\n",
            "Train Epoch 34: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0155]\n",
            "Test Epoch 34: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0152]\n",
            "Train Epoch 35: 100%|██████████| 126/126 [01:39<00:00,  1.26it/s, Loss 0.0154]\n",
            "Test Epoch 35: 100%|██████████| 32/32 [00:25<00:00,  1.28it/s, Test Loss 0.0153]\n"
          ]
        }
      ],
      "source": [
        "# Training Process\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "EPOCHS = 35\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, kor_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss = train_step(kor_train[idx:idx+BATCH_SIZE],\n",
        "                                eng_train[idx:idx+BATCH_SIZE],\n",
        "                                encoder,\n",
        "                                decoder,\n",
        "                                optimizer,\n",
        "                                eng_tokenizer)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Train Epoch %2d' % (epoch +1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy()/ (batch+1)))\n",
        "\n",
        "    test_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, kor_val.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (test_batch, idx) in enumerate(t):\n",
        "        test_batch_loss = eval_step(kor_val[idx:idx+BATCH_SIZE],\n",
        "                                    eng_val[idx:idx+BATCH_SIZE],\n",
        "                                    encoder,\n",
        "                                    decoder,\n",
        "                                    optimizer,\n",
        "                                    eng_tokenizer)\n",
        "        test_loss += test_batch_loss\n",
        "\n",
        "        t.set_description_str('Test Epoch %2d' % (epoch+1))\n",
        "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy()/ (test_batch+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9bl-XCj9pA"
      },
      "source": [
        "#18.새로운 입력으로 들어오는 문장 정리 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRPYzVN0innl"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "def evaluate(sentence, encoder, decoder):\n",
        "    attention = np.zeros((eng_train.shape[-1], kor_train.shape[-1]))\n",
        "    sentence = preprocess_sentence(sentence, token=False)\n",
        "\n",
        "    inputs = kor_tokenizer.texts_to_sequences(mecab.morphs(sentence))\n",
        "    flatten = [list(chain(*inputs))]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(flatten, maxlen = kor_train.shape[-1], padding= 'post')\n",
        "  \n",
        "    result = ''\n",
        "    enc_out = encoder(inputs)\n",
        "    dec_hidden = enc_out[:, -1]\n",
        "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(eng_train.shape[-1]):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                            dec_hidden,\n",
        "                                                            enc_out)\n",
        "    \n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
        "\n",
        "        result += eng_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if eng_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyhbe50nkeeB"
      },
      "source": [
        "#19. 문장 일치 비교 그래프 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84CTLFVIiqLT"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.ticker as ticker\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_attention(attention, sentence, predicted_sentence):\n",
        "\n",
        "#     plt.rcParams['font.family'] ='Malgun Gothic'\n",
        "#     plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "#     fig = plt.figure(figsize=(10, 10))\n",
        "#     ax = fig.add_subplot(1, 1, 1)\n",
        "#     ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "#     fontdict = {'fontsize': 14}\n",
        "\n",
        "#     ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "#     ax.set_yticklabels([''] + predicted_sentence, fontdict = fontdict)\n",
        "\n",
        "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20.번역 확인 함수"
      ],
      "metadata": {
        "id": "TdZPe-kYgM2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50bxk-ZeiwT1"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, encoder, decoder):\n",
        "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
        "\n",
        "    print('Input : %s' % (sentence))\n",
        "    print('Output : {}'.format(result),'\\n')\n",
        "\n",
        "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
        "    # plot_attention(attention, sentence.split(), result.split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['그는 미국의 대통령',\n",
        "            '그녀는 휴식이 필요해',\n",
        "            '이스라엘은 이란과 전쟁을 준비한다',\n",
        "            '지구의 날씨는 점점 위험하다',\n",
        "            '미군은 이라크에서 철수 했다']\n",
        "\n",
        "for word in sentence:\n",
        "    translate(str(word), encoder, decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p-zWRE4e5U1",
        "outputId": "7ef58818-d120-4df2-900b-6c3891375226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : 그는 미국의 대통령\n",
            "Output : he is the americans are <end>  \n",
            "\n",
            "Input : 그녀는 휴식이 필요해\n",
            "Output : she was at home <end>  \n",
            "\n",
            "Input : 이스라엘은 이란과 전쟁을 준비한다\n",
            "Output : israel considers iran for terror <end>  \n",
            "\n",
            "Input : 지구의 날씨는 점점 위험하다\n",
            "Output : the weather on earth could face <end>  \n",
            "\n",
            "Input : 미군은 이라크에서 철수 했다\n",
            "Output : the us military is to us troops in iraq <end>  \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "seq_2_seq 구현",
      "provenance": [],
      "authorship_tag": "ABX9TyMbeuq8kBTVcD3DQFcqClBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}